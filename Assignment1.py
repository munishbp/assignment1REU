# -*- coding: utf-8 -*-
"""Assignment 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qv1GVY1IS8G5LJHv_ttLdOpFPnPeE3oy

My goal with CIFAR is to employ "self made" transformers in order to practice my PyTorch skills and reinforce my Transformers knowledge. I need to take smaller tiles of each image, 3 per image for RGB.

Take 4x4 patches bc each image is 32x32.
That means we get 64 vectors because the 32 by 32 image turns into 8 4x4 tiles.
8 down 8 across.
Due to each patch being 4x4x3, we would get 64 vectors with 48 dimensions per image.

Encoder only due to only needing to classify.
Image->patches (each patch is a token and the tokens go in as the sequence)->positional encoding->Transformer encoding (self attention)->Output (just one as a special CLS token due to self attention)->Linear (maps to classification) classification->Softmax->Class prediction

CLS(classification) token backtracks to the other tokens and gets how relevant it is to what's being asked and the CLS aggregates this information. After multiple layers, CLS has a global view. This allows us to use one vector for classification. CLS only has one job to gather this information as an aggregate.
"""

!pip install transformers
!pip install datasets
!pip install -U datasets huggingface_hub fsspec
!pip3 install torch torchaudio torchvision torchtext torchdata
import torchvision
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from huggingface_hub import notebook_login
from datasets import load_dataset
notebook_login()

ds = load_dataset("uoft-cs/cifar100")

#features in train are img, fine_label, and coarse_label
#features in test are img, fine_label, and coarse_label

#patch embeddings

class PatchEmbedding(nn.Module):
  def __init__(self, img_size=32,patch_size=4,in_channels=3,embed_dim=512):
    super().__init__()
    self.img_size=img_size
    self.patch_size=patch_size
    self.n_patches=(img_size//patch_size)**2

    self.proj=nn.Conv2d(in_channels=3,out_channels=embed_dim, kernel_size=patch_size,stride=patch_size)

    #positional embedding, need +1 for CLS token
    self.positional_embedding=nn.Parameter(torch.zeros(1,self.n_patches+1,embed_dim))

    #CLS token
    self.cls_token=nn.Parameter(torch.zeros(1,1,embed_dim))

  #x is batch of images with the shape(batch_size, channels, height, and width)
  #input is (batch_size,3,32,32)
  #output is (batch_size,512,8,8)
  def forward(self,x):
    batch_size = x.shape[0]
    #patch embedding
    x=self.proj(x)
    #flattens after dimension 2, i only need the 64 patches
    x=x.flatten(2)
    #swaps dimenions 1 and 2 in x
    x=x.transpose(1,2)

    #CLS token added to beginning
    cls_tokens=self.cls_token.expand(batch_size,-1,-1) #allows us to view contents without copying underlying data
    x=torch.cat((cls_tokens,x),dim=1) #(batch,65 bc of CLS, 512)

    #positional embedding
    x=x+self.positional_embedding


    return x

#goes from CNN-style(batch,channels,height,width) to Transformer(batch,sequence,features)

class MultiHeadSelfAttention(nn.Module):
  def __init__(self,embed_dim=512,num_heads=8):
    super().__init__()
    self.embed_dim=embed_dim
    self.num_heads=num_heads
    self.head_dim=embed_dim//num_heads #64

    self.qkv=nn.Linear(embed_dim,3*embed_dim) #times 3 for the projections of q,k,v
    self.proj=nn.Linear(embed_dim,embed_dim)

  def forward(self,x):
    batch_size,seq_len,embed_dim=x.shape

    #generate q,k,v
    qkv=self.qkv(x) #shape is (batch,seq_len,embed_dim*3)
    qkv=qkv.reshape(batch_size,seq_len,3,self.num_heads,self.head_dim)
    qkv=qkv.permute(2,0,3,1,4) #(3,batch,heads,seq_len,head_dim)
    q,k,v=qkv[0],qkv[1],qkv[2]

    #softmax
    scores=torch.matmul(q,k.transpose(-2,-1))
    scores=scores/torch.sqrt(torch.tensor(self.head_dim))
    attention_weights=F.softmax(scores,dim=-1)#last dimension, we softmax here bc each query position distributes its attention across all key positions.
    #basically normalizing across all key positions

    out=torch.matmul(attention_weights,v)

    out=out.transpose(1,2)
    out=out.reshape(batch_size,seq_len,embed_dim)
    out=self.proj(out)

    return out

class TransformerEncoder(nn.Module):
  def __init__(self, embed_dim=512, num_heads=8, mlp_ratio=4, dropout=0.1):
    super().__init__()

    #self attention
    self.mhsa=MultiHeadSelfAttention(embed_dim,num_heads)

    #layer1 normalization
    self.norm1=nn.LayerNorm(embed_dim)
    #layer2 normalization
    self.norm2=nn.LayerNorm(embed_dim)

    #feedforward component-MLP
    mlp_dim=int(embed_dim*mlp_ratio)  #512x4=2048
    self.mlp=nn.Sequential(nn.Linear(embed_dim,mlp_dim),nn.GELU(),nn.Linear(mlp_dim,embed_dim))#project up, activation function, project down


    #dropout for regularization
    self.dropout=nn.Dropout(dropout)

    #takes output from self attention and turns into final output for model. implementation is through MLP
    #learns the nonlinear relationships, parallelizable bc computations can be performed independently from others in the layer
    #feed forward can be employed in a CNN

  def forward(self,x):
    #attention block with residual connection


    #step 1: attention with residual
    attention_out=self.mhsa(self.norm1(x))
    x=x+self.dropout(attention_out)


    #step 2: MLP with residual
    mlp_out=self.mlp(self.norm2(x))
    x=x+self.dropout(mlp_out)
    return x
