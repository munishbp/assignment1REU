# -*- coding: utf-8 -*-
"""Assignment 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qv1GVY1IS8G5LJHv_ttLdOpFPnPeE3oy

My goal with CIFAR is to employ "self made" transformers in order to practice my PyTorch skills and reinforce my Transformers knowledge. I need to take smaller tiles of each image, 3 per image for RGB.

Take 4x4 patches bc each image is 32x32.
That means we get 64 vectors because the 32 by 32 image turns into 8 4x4 tiles.
8 down 8 across.
Due to each patch being 4x4x3, we would get 64 vectors with 48 dimensions per image.

Encoder only due to only needing to classify.
Image->patches (each patch is a token and the tokens go in as the sequence)->positional encoding->Transformer encoding (self attention)->Output (just one as a special CLS token due to self attention)->Linear (maps to classification) classification->Softmax->Class prediction

CLS(classification) token backtracks to the other tokens and gets how relevant it is to what's being asked and the CLS aggregates this information. After multiple layers, CLS has a global view. This allows us to use one vector for classification. CLS only has one job to gather this information as an aggregate.
"""

!pip install transformers
!pip install datasets
!pip install -U datasets huggingface_hub fsspec
!pip3 install torch torchaudio torchvision torchtext torchdata
import torchvision
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from huggingface_hub import notebook_login
from datasets import load_dataset
notebook_login()

ds = load_dataset("uoft-cs/cifar100")

#features in train are img, fine_label, and coarse_label
#features in test are img, fine_label, and coarse_label

#patch embeddings

class PatchEmbedding(nn.Module):
  def __init__(self, img_size=32,patch_size=4,in_channels=3,embed_dim=512):
    super().__init__()
    self.img_size=img_size
    self.patch_size=patch_size
    self.n_patches=(img_size//patch_size)**2

    self.proj=nn.Conv2d(in_channels=3,out_channels=embed_dim, kernel_size=patch_size,stride=patch_size)

  #x is batch of images with the shape(batch_size, channels, height, and width)
  #input is (batch_size,3,32,32)
  #output is (batch_size,512,8,8)
  def forward(self,x):
    x=self.proj(x)
    print(f"After convolution:{x.shape}")

    #flattens after dimension 2, i only need the 64 patches
    x=x.flatten(2)
    #swaps dimenions 1 and 2 in x
    x=x.transpose(1,2)


    return x

#Goes from CNN-style(batch,channels,height,width) to Transformer(batch,sequence,features)

#positional encodings
