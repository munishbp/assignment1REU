# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QFiQuDUezWh7LajFAIGsz_LO8xk7GuqE

My goal with CIFAR is to employ "self made" transformers in order to practice my PyTorch skills and reinforce my Transformers knowledge. I need to take smaller tiles of each image, 3 per image for RGB.

Take 4x4 patches bc each image is 32x32.
That means we get 64 vectors because the 32 by 32 image turns into 8 4x4 tiles.
8 down 8 across.
Due to each patch being 4x4x3, we would get 64 vectors with 48 dimensions per image.

Encoder only due to only needing to classify.
Image->patches (each patch is a token and the tokens go in as the sequence)->positional encoding->Transformer encoding (self attention)->Output (just one as a special CLS token due to self attention)->Linear (maps to classification) classification->Softmax->Class prediction

CLS(classification) token backtracks to the other tokens and gets how relevant it is to what's being asked and the CLS aggregates this information. After multiple layers, CLS has a global view. This allows us to use one vector for classification. CLS only has one job to gather this information as an aggregate.
"""

!pip install transformers
!pip install datasets
!pip install -U datasets huggingface_hub fsspec
!pip3 install torch torchaudio torchvision torchtext torchdata
import torchvision
import torchvision.transforms as transforms
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import time
from collections import defaultdict

#features in train are img, fine_label, and coarse_label
#features in test are img, fine_label, and coarse_label

#patch embeddings

class PatchEmbedding(nn.Module):
  def __init__(self, img_size=32,patch_size=4,in_channels=3,embed_dim=512):
    super().__init__()
    self.img_size=img_size
    self.patch_size=patch_size
    self.n_patches=(img_size//patch_size)**2

    self.proj=nn.Conv2d(in_channels=3,out_channels=embed_dim, kernel_size=patch_size,stride=patch_size)

    #positional embedding, need +1 for CLS token
    self.positional_embedding=nn.Parameter(torch.zeros(1,self.n_patches+1,embed_dim))

    #CLS token
    self.cls_token=nn.Parameter(torch.zeros(1,1,embed_dim))

  #x is batch of images with the shape(batch_size, channels, height, and width)
  #input is (batch_size,3,32,32)
  #output is (batch_size,512,8,8)
  def forward(self,x):
    batch_size = x.shape[0]
    #patch embedding
    x=self.proj(x)
    #flattens after dimension 2, i only need the 64 patches
    x=x.flatten(2)
    #swaps dimenions 1 and 2 in x
    x=x.transpose(1,2)

    #CLS token added to beginning
    cls_tokens=self.cls_token.expand(batch_size,-1,-1) #allows us to view contents without copying underlying data
    x=torch.cat((cls_tokens,x),dim=1) #(batch,65 bc of CLS, 512)

    #positional embedding
    x=x+self.positional_embedding


    return x

#goes from CNN-style(batch,channels,height,width) to Transformer(batch,sequence,features)

class MultiHeadSelfAttention(nn.Module):
  def __init__(self,embed_dim=512,num_heads=8):
    super().__init__()
    self.embed_dim=embed_dim
    self.num_heads=num_heads
    self.head_dim=embed_dim//num_heads #64

    self.qkv=nn.Linear(embed_dim,3*embed_dim) #times 3 for the projections of q,k,v
    self.proj=nn.Linear(embed_dim,embed_dim)

  def forward(self,x):
    batch_size,seq_len,embed_dim=x.shape

    #generate q,k,v
    qkv=self.qkv(x) #shape is (batch,seq_len,embed_dim*3)
    qkv=qkv.reshape(batch_size,seq_len,3,self.num_heads,self.head_dim)
    qkv=qkv.permute(2,0,3,1,4) #(3,batch,heads,seq_len,head_dim)
    q,k,v=qkv[0],qkv[1],qkv[2]

    #softmax
    scores=torch.matmul(q,k.transpose(-2,-1))
    scores=scores/torch.sqrt(torch.tensor(self.head_dim))
    attention_weights=F.softmax(scores,dim=-1)#last dimension, we softmax here bc each query position distributes its attention across all key positions.
    #basically normalizing across all key positions

    out=torch.matmul(attention_weights,v)

    out=out.transpose(1,2)
    out=out.reshape(batch_size,seq_len,embed_dim)
    out=self.proj(out)

    return out

class TransformerEncoder(nn.Module):
  def __init__(self, embed_dim=512, num_heads=8, mlp_ratio=4, dropout=0.1):
    super().__init__()

    #self attention
    self.mhsa=MultiHeadSelfAttention(embed_dim,num_heads)

    #layer1 normalization
    self.norm1=nn.LayerNorm(embed_dim)
    #layer2 normalization
    self.norm2=nn.LayerNorm(embed_dim)

    #feedforward component-MLP
    mlp_dim=int(embed_dim*mlp_ratio)  #512x4=2048
    self.mlp=nn.Sequential(nn.Linear(embed_dim,mlp_dim),nn.GELU(),nn.Linear(mlp_dim,embed_dim))#project up, activation function, project down


    #dropout for regularization
    self.dropout=nn.Dropout(dropout)

    #takes output from self attention and turns into final output for model. implementation is through MLP
    #learns the nonlinear relationships, parallelizable bc computations can be performed independently from others in the layer
    #feed forward can be employed in a CNN

  def forward(self,x):
    #attention block with residual connection


    #step 1: attention with residual
    attention_out=self.mhsa(self.norm1(x))
    x=x+self.dropout(attention_out)


    #step 2: MLP with residual
    mlp_out=self.mlp(self.norm2(x))
    x=x+self.dropout(mlp_out)
    return x

class VisionTransformer(nn.Module):
  def __init__(self, img_size=32,patch_size=4,num_classes=100,embed_dim=512,depth=6,num_heads=8,mlp_ratio=4):
    super().__init__()


    #patch with positional encoding
    self.patch_embed=PatchEmbedding(img_size,patch_size,3,embed_dim)

    #6 transformer blocks
    self.blocks=nn.ModuleList([TransformerEncoder(embed_dim,num_heads,mlp_ratio) for _ in range(depth)])

    #final layer for normalization
    self.norm=nn.LayerNorm(embed_dim)

    #classification
    self.head=nn.Linear(embed_dim,num_classes)

  def forward(self,x):
    #convert to patches and add positional encoding
    x=self.patch_embed(x)

    #goes through every transformer
    for block in self.blocks:
      x=block(x)

    #final normalization after transformers
    x=self.norm(x)


    #gets the CLS token first for classification
    cls_token=x[:,0] #(batch,embed dimensions)

    logits=self.head(cls_token)

    return logits

#load in CIFAR-100
# data augmentation and normalization
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5071, 0.4867, 0.4408),
                       (0.2675, 0.2565, 0.2761))  # CIFAR-100 stats
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5071, 0.4867, 0.4408),
                       (0.2675, 0.2565, 0.2761))
])

#load dataset
trainset = torchvision.datasets.CIFAR100(
    root='./data', train=True, download=True, transform=transform_train)
trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR100(
    root='./data', train=False, download=True, transform=transform_test)
testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)

print(f"Training samples: {len(trainset)}")
print(f"Test samples: {len(testset)}")

#initialzing model
model = VisionTransformer(
    img_size=32,
    patch_size=4,
    num_classes=100,
    embed_dim=512,
    depth=6,
    num_heads=8
)


#parameter counter
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")

#training setup
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
import time

#moving model to gpu
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
print(f"Using device: {device}")

#loss function
criterion = nn.CrossEntropyLoss()

#AdamW optimizer-SGD
optimizer = optim.AdamW(
    model.parameters(),
    lr=1e-3,  #learning rate
    weight_decay=0.05  #L2 regularization for overfitting prevention
)

#learning rate scheduler
num_epochs = 100
scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)

#training function
def train_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (inputs, labels) in enumerate(dataloader):
        inputs, labels = inputs.to(device), labels.to(device)

        #zero gradients
        optimizer.zero_grad()

        #forward
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        #backward
        loss.backward()
        optimizer.step()

        #statistics
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        #progress update every 100 batches
        if batch_idx % 100 == 0:
            print(f'Batch [{batch_idx}/{len(dataloader)}] '
                  f'Loss: {loss.item():.4f} '
                  f'Acc: {100.*correct/total:.2f}%')

    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

#validation
def validate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

#complete training loop

train_losses = []
train_accs = []
val_losses = []
val_accs = []

#best model
best_val_acc = 0.0

print("Starting training...")
for epoch in range(num_epochs):
    start_time = time.time()

    #train
    train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)

    #validate
    val_loss, val_acc = validate(model, testloader, criterion, device)

    #update learning rate
    scheduler.step()

    #track history
    train_losses.append(train_loss)
    train_accs.append(train_acc)
    val_losses.append(val_loss)
    val_accs.append(val_acc)

    #saving the best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_vit_cifar100.pth')

    #end of training print statements
    epoch_time = time.time() - start_time
    print(f'\nEpoch [{epoch+1}/{num_epochs}] ({epoch_time:.2f}s)')
    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')
    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')
    print(f'Best Val Acc: {best_val_acc:.2f}%')
    print('-' * 60)

"""starting the CNN portion from here. each layer learns a different feature. last layer combines all that feature data to say "this is probably a cat".
transformers have a global view because of CLS token. CNNs are more local

"""

#applies convolutions to detect features, uses a ReLU activation.
#basic example
class CNNBlock(nn.Module):
  def __init__(self,in_channels,out_channels):
    super().__init__()
    self.conv=nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)
    self.bn=nn.BatchNorm2d(out_channels) #normalizes output
    self.relu=nn.ReLU() #activation function
    self.pool=nn.MaxPool2d(2,2) #reduce size by 2

  def forward(self,x):
    x=self.conv(x)
    x=self.bn(x)
    x=self.relu(x)
    x=self.pool(x)
    return x

#better cnn block with stride control
class BetterCNN(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(BetterCNN, self).__init__()
        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=3,
            padding=1,
            stride=stride
        )
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        x = self.pool(x)
        return x

#full cnn architecture for cifar-100
class CIFAR100CNN(nn.Module):
    def __init__(self, num_classes):
        super(CIFAR100CNN, self).__init__()
        #stacked convolutional blocks - each one learns different features
        self.layer1 = CNNBlock(in_channels=3, out_channels=64)
        self.layer2 = CNNBlock(in_channels=64, out_channels=128)
        self.layer3 = CNNBlock(in_channels=128, out_channels=256)
        self.layer4 = CNNBlock(in_channels=256, out_channels=512)

        #after four pooling operations on 32×32 input, the feature map is 2×2
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(512 * 2 * 2, 1024)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(1024, num_classes)

    def forward(self, x):
        x = self.layer1(x)      #(batch, 64, 16, 16)
        x = self.layer2(x)      #(batch, 128, 8, 8)
        x = self.layer3(x)      #(batch, 256, 4, 4)
        x = self.layer4(x)      #(batch, 512, 2, 2)
        x = self.flatten(x)     #(batch, 512*2*2)
        x = self.fc1(x)         #(batch, 1024)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)         #(batch, num_classes)
        return x

#cnn training function
def train_cnn_optimized(
    model,
    train_loader,
    val_loader,
    device='cuda',
    epochs=10,
    lr=1e-3
):
    model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []

    for epoch in range(epochs):
        #training step
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for images, labels in train_loader:
            images = images.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)

        epoch_loss = running_loss / total
        epoch_acc = correct / total
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc)

        #validation step
        model.eval()
        val_running_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_running_loss += loss.item() * images.size(0)
                _, predicted = outputs.max(1)
                val_correct += predicted.eq(labels).sum().item()
                val_total += labels.size(0)

        val_epoch_loss = val_running_loss / val_total
        val_epoch_acc = val_correct / val_total
        val_losses.append(val_epoch_loss)
        val_accs.append(val_epoch_acc)

        print(
            f"Epoch {epoch+1}/{epochs}, "
            f"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, "
            f"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}"
        )

    return {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accs': train_accs,
        'val_accs': val_accs
    }

#compare the results between cnn and vision transformer
class ModelComparison:
    #compare cnn and vision transformer on various metrics

    def __init__(self, cnn_model, transformer_model, testloader, device='cuda'):
        self.cnn = cnn_model.to(device)
        self.transformer = transformer_model.to(device)
        self.testloader = testloader
        self.device = device
        self.results = {}

    def evaluate_accuracy_per_class(self, model, model_name):
        #get per-class accuracy for understanding which classes each model does well on
        model.eval()
        class_correct = defaultdict(int)
        class_total = defaultdict(int)
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for inputs, labels in self.testloader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

                for pred, label in zip(predicted, labels):
                    class_correct[label.item()] += (pred == label).item()
                    class_total[label.item()] += 1

        #calculate per-class accuracy
        per_class_acc = {}
        for class_id in range(100):
            if class_total[class_id] > 0:
                per_class_acc[class_id] = 100.0 * class_correct[class_id] / class_total[class_id]
            else:
                per_class_acc[class_id] = 0.0

        self.results[f'{model_name}_per_class_acc'] = per_class_acc
        self.results[f'{model_name}_predictions'] = all_preds
        self.results[f'{model_name}_labels'] = all_labels

        return per_class_acc

    def measure_inference_speed(self, model, model_name, num_runs=100):
        #measure how fast each model processes images
        model.eval()

        #warmup - helps get more accurate timing, reduce time of GPU kernels, and preallocate memory
        dummy_input = torch.randn(1, 3, 32, 32).to(self.device)
        for _ in range(10):
            _ = model(dummy_input)

        #measure batch inference
        batch_times = []
        with torch.no_grad():
            for i, (inputs, _) in enumerate(self.testloader):
                if i >= num_runs:
                    break

                inputs = inputs.to(self.device)
                torch.cuda.synchronize() if self.device == 'cuda' else None

                start_time = time.time()
                _ = model(inputs)
                torch.cuda.synchronize() if self.device == 'cuda' else None

                batch_times.append(time.time() - start_time)

        avg_batch_time = np.mean(batch_times)
        std_batch_time = np.std(batch_times)

        #measure single image inference
        single_times = []
        with torch.no_grad():
            for _ in range(num_runs):
                torch.cuda.synchronize() if self.device == 'cuda' else None
                start_time = time.time()
                _ = model(dummy_input)
                torch.cuda.synchronize() if self.device == 'cuda' else None
                single_times.append(time.time() - start_time)

        avg_single_time = np.mean(single_times) * 1000  #convert to ms

        self.results[f'{model_name}_avg_batch_time'] = avg_batch_time
        self.results[f'{model_name}_std_batch_time'] = std_batch_time
        self.results[f'{model_name}_avg_single_time_ms'] = avg_single_time

        return avg_batch_time, avg_single_time

    def analyze_robustness(self, model, model_name):
        #test how well models handle noisy inputs
        model.eval()

        #test with different noise levels
        noise_levels = [0.01, 0.05, 0.1, 0.2]
        accuracies = []

        for noise_level in noise_levels:
            correct = 0
            total = 0

            with torch.no_grad():
                for inputs, labels in self.testloader:
                    inputs, labels = inputs.to(self.device), labels.to(self.device)

                    #add gaussian noise to see how robust the model is
                    noise = torch.randn_like(inputs) * noise_level
                    noisy_inputs = inputs + noise
                    noisy_inputs = torch.clamp(noisy_inputs, 0, 1)

                    outputs = model(noisy_inputs)
                    _, predicted = outputs.max(1)

                    total += labels.size(0)
                    correct += predicted.eq(labels).sum().item()

            accuracy = 100. * correct / total
            accuracies.append(accuracy)

        self.results[f'{model_name}_robustness'] = dict(zip(noise_levels, accuracies))
        return accuracies

    def count_parameters(self, model, model_name):
        #count model parameters to understand model size
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        self.results[f'{model_name}_total_params'] = total_params
        self.results[f'{model_name}_trainable_params'] = trainable_params

        return total_params, trainable_params

    def plot_comparison(self):
        #create comprehensive comparison plots
        fig = plt.figure(figsize=(20, 12))

        #overall accuracy comparison
        ax1 = plt.subplot(2, 3, 1)
        cnn_acc = np.mean(list(self.results['CNN_per_class_acc'].values()))
        trans_acc = np.mean(list(self.results['Transformer_per_class_acc'].values()))

        models = ['CNN', 'Transformer']
        accuracies = [cnn_acc, trans_acc]
        colors = ['#3498db', '#e74c3c']

        bars = ax1.bar(models, accuracies, color=colors, alpha=0.8)
        ax1.set_ylabel('Accuracy (%)')
        ax1.set_title('Overall Accuracy Comparison')
        ax1.set_ylim(0, 100)

        #add value labels on bars
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{acc:.2f}%', ha='center', va='bottom')

        #inference speed comparison
        ax2 = plt.subplot(2, 3, 2)
        cnn_speed = self.results['CNN_avg_single_time_ms']
        trans_speed = self.results['Transformer_avg_single_time_ms']

        speeds = [cnn_speed, trans_speed]
        bars = ax2.bar(models, speeds, color=colors, alpha=0.8)
        ax2.set_ylabel('Inference Time (ms)')
        ax2.set_title('Single Image Inference Speed')

        for bar, speed in zip(bars, speeds):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{speed:.2f}ms', ha='center', va='bottom')

        #parameter count comparison
        ax3 = plt.subplot(2, 3, 3)
        cnn_params = self.results['CNN_total_params'] / 1e6  #convert to millions
        trans_params = self.results['Transformer_total_params'] / 1e6

        params = [cnn_params, trans_params]
        bars = ax3.bar(models, params, color=colors, alpha=0.8)
        ax3.set_ylabel('Parameters (Millions)')
        ax3.set_title('Model Size Comparison')

        for bar, param in zip(bars, params):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{param:.2f}M', ha='center', va='bottom')

        #robustness comparison
        ax4 = plt.subplot(2, 3, 4)
        noise_levels = list(self.results['CNN_robustness'].keys())
        cnn_robust = list(self.results['CNN_robustness'].values())
        trans_robust = list(self.results['Transformer_robustness'].values())

        ax4.plot(noise_levels, cnn_robust, 'o-', color=colors[0],
                label='CNN', linewidth=2, markersize=8)
        ax4.plot(noise_levels, trans_robust, 's-', color=colors[1],
                label='Transformer', linewidth=2, markersize=8)
        ax4.set_xlabel('Noise Level')
        ax4.set_ylabel('Accuracy (%)')
        ax4.set_title('Robustness to Gaussian Noise')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        #per-class accuracy distribution
        ax5 = plt.subplot(2, 3, 5)
        cnn_class_accs = list(self.results['CNN_per_class_acc'].values())
        trans_class_accs = list(self.results['Transformer_per_class_acc'].values())

        ax5.hist(cnn_class_accs, bins=20, alpha=0.6, color=colors[0],
                label='CNN', edgecolor='black')
        ax5.hist(trans_class_accs, bins=20, alpha=0.6, color=colors[1],
                label='Transformer', edgecolor='black')
        ax5.set_xlabel('Per-Class Accuracy (%)')
        ax5.set_ylabel('Number of Classes')
        ax5.set_title('Distribution of Per-Class Accuracies')
        ax5.legend()

        #efficiency score (accuracy per million parameters)
        ax6 = plt.subplot(2, 3, 6)
        cnn_efficiency = cnn_acc / (self.results['CNN_total_params'] / 1e6)
        trans_efficiency = trans_acc / (self.results['Transformer_total_params'] / 1e6)

        efficiencies = [cnn_efficiency, trans_efficiency]
        bars = ax6.bar(models, efficiencies, color=colors, alpha=0.8)
        ax6.set_ylabel('Accuracy per Million Parameters')
        ax6.set_title('Model Efficiency')

        for bar, eff in zip(bars, efficiencies):
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{eff:.2f}', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig('cnn_vs_transformer_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()

        return fig


#run the comprehensive comparison
def run_full_comparison(cnn_model, transformer_model, testloader, device='cuda'):
    #execute complete model comparison
    print("Starting comprehensive model comparison...")

    comparison = ModelComparison(cnn_model, transformer_model, testloader, device)

    #evaluate accuracy
    print("\n1. Evaluating per-class accuracy...")
    comparison.evaluate_accuracy_per_class(cnn_model, 'CNN')
    comparison.evaluate_accuracy_per_class(transformer_model, 'Transformer')

    #measure inference speed
    print("\n2. Measuring inference speed...")
    comparison.measure_inference_speed(cnn_model, 'CNN')
    comparison.measure_inference_speed(transformer_model, 'Transformer')

    #test robustness
    print("\n3. Testing robustness to noise...")
    comparison.analyze_robustness(cnn_model, 'CNN')
    comparison.analyze_robustness(transformer_model, 'Transformer')

    #count parameters
    print("\n4. Counting parameters...")
    comparison.count_parameters(cnn_model, 'CNN')
    comparison.count_parameters(transformer_model, 'Transformer')

    #generate visualizations
    print("\n5. Creating comparison plots...")
    comparison.plot_comparison()


    return comparison.results

#execution script that handles missing checkpoint files

import os

#check device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

#initialize cnn model
print("\n" + "="*60)
print("INITIALIZING CNN MODEL")
print("="*60)
cnn_model = CIFAR100CNN(num_classes=100)

#count parameters
cnn_params = sum(p.numel() for p in cnn_model.parameters())
transformer_params = sum(p.numel() for p in model.parameters())
print(f"CNN Total parameters: {cnn_params:,}")
print(f"Transformer Total parameters: {transformer_params:,}")

#train cnn (or load if already trained)
if not os.path.exists('best_cnn_cifar100.pth'):
    print("\n" + "="*60)
    print("TRAINING CNN MODEL")
    print("="*60)
    cnn_results = train_cnn_optimized(
        cnn_model,
        trainloader,
        testloader,
        epochs=100,
        device=device
    )
else:
    print("\nCNN checkpoint found, loading trained model...")
    checkpoint = torch.load('best_cnn_cifar100.pth', map_location=device)
    cnn_model.load_state_dict(checkpoint['model_state_dict'])
    print(f"Loaded CNN with validation accuracy: {checkpoint['best_val_acc']:.2f}%")

#load the transformer model (rename for clarity)
transformer_model = model  #your existing vision transformer

#check if transformer checkpoint exists
if os.path.exists('best_vit_cifar100.pth'):
    print("\nLoading best Vision Transformer checkpoint...")
    transformer_model.load_state_dict(torch.load('best_vit_cifar100.pth', map_location=device))
else:
    print("\nWarning: No transformer checkpoint found. Using current model state.")
    print("Make sure your transformer has been trained!")

#run comprehensive comparison
print("\n" + "="*60)
print("RUNNING MODEL COMPARISON")
print("="*60)

#ensure both models are in eval mode and on correct device
cnn_model = cnn_model.to(device)
transformer_model = transformer_model.to(device)

comparison_results = run_full_comparison(
    cnn_model,
    transformer_model,
    testloader,
    device=device
)

print("\n" + "="*60)
print("COMPARISON COMPLETE")
print("="*60)

#quick summary
if 'CNN_per_class_acc' in comparison_results and 'Transformer_per_class_acc' in comparison_results:
    cnn_acc = np.mean(list(comparison_results['CNN_per_class_acc'].values()))
    trans_acc = np.mean(list(comparison_results['Transformer_per_class_acc'].values()))

    print(f"\nQuick Summary:")
    print(f"CNN Accuracy: {cnn_acc:.2f}%")
    print(f"Transformer Accuracy: {trans_acc:.2f}%")
    print(f"CNN Parameters: {comparison_results['CNN_total_params']:,}")
    print(f"Transformer Parameters: {comparison_results['Transformer_total_params']:,}")
    print(f"CNN Inference Speed: {comparison_results['CNN_avg_single_time_ms']:.2f}ms")
    print(f"Transformer Inference Speed: {comparison_results['Transformer_avg_single_time_ms']:.2f}ms")
